---
title: "I2DS Tools for Data Science Workshop 2023: Tidy modeling with tidymodels in R"
authors: "Daniyar Imanaliev, Henry Baker, Isabella Urbano Trujillo"
output: 
  html_document:
    toc: TRUE
    df_print: paged
    number_sections: FALSE
    highlight: tango
    theme: lumen
    toc_depth: 3
    toc_float: true
    css: custom.css 
    self_contained: false
    
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      eval = TRUE,
                      error = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      comment = NA)
```

# What is tidymodels?

Tidymodels is a collection of R packages and a framework for modeling and machine learning that follows the principles of tidy data and integrates seamlessly with the tidyverse ecosystem. It was developed to provide a consistent and organized way to perform machine learning tasks in R, making it easier for data scientists and analysts to build, evaluate, and deploy predictive models.

Install tidymodels with:
```{r, include = T}
# install.packages("tidymodels")
library(tidymodels)
```

# Why tidymodels and not other R package?

Tidymodels offers several advantages that may make it a suitable choice for many data scientists and analysts. Here are some reasons why Tidymodels is often preferred:

* Tidyverse Integration: Tidymodels is built on the principles of the tidyverse, which promotes consistent and user-friendly data manipulation. If you are already familiar with the tidyverse, using Tidymodels can provide a seamless and consistent experience throughout your data analysis and modeling workflow.
* Consistency and Workflow: Tidymodels provides a structured workflow for modeling, including data pre-processing, model specification, tuning, and evaluation. This consistency can make your modeling process more organized and transparent.
* Recipes for Data Pre-processing: Tidymodels includes the recipes package, which allows you to specify data pre-processing steps in a structured and reproducible manner. This is particularly helpful for feature engineering and data transformation.
* Model Agnosticism: Tidymodels is model-agnostic, meaning it allows you to work with various modeling algorithms without significant changes to your code. You can easily swap out different models for experimentation and model selection.
* Hyperparameter Tuning: Tidymodels offers tools for hyperparameter tuning and model selection, making it easier to find the best combination of hyperparameters for your models.
* Resampling and Cross-Validation: The framework provides resampling methods for assessing model performance, such as cross-validation and bootstrapping, which are critical for estimating model generalization.
* Extensive Metrics: Tidymodels includes the yardstick package with a wide range of evaluation metrics for various types of models, making it easier to compare and assess model performance.
* Consistent API: The parsnip package provides a consistent interface for specifying and fitting models, regardless of the underlying modeling package. This can simplify the process of trying different models.
* Reproducibility and Documentation: Tidymodels emphasizes good practices for reproducibility, making it easier to document and share your modeling workflows with colleagues.
* Active Development: Tidymodels is actively maintained and updated, ensuring that it stays relevant and up-to-date with the latest developments in the field of machine learning.

# How does modeling fit into the data analysis process? 

According to Wickham and Grolemund (2016) the following figure illustrates the general data analysis process. Data ingestion and cleaning/tidying are shown as the initial steps. When the analytical steps for understanding commence, they are a heuristic process; we cannot pre-determine how long they may take. The cycle of transformation, modeling, and visualization often requires multiple iterations.

```{r, fig.align='center', echo=F, out.width = "90%"}
knitr::include_graphics("pics/data_process.png")
```
# Types of models

* Descriptive Models: these aim to summarize and describe data to gain insights into its characteristics. They are used for exploratory data analysis (EDA) and data visualization to understand the underlying patterns, relationships, and distributions in the data.Examples: Histograms, bar charts, scatter plots, summary statistics (mean, median, standard deviation), and data tables are common tools used for descriptive modeling.

* Inferential Models: these are used to draw conclusions or make inferences about a population based on a sample of data.They are employed in hypothesis testing and statistical analysis to determine whether observed effects are statistically significant and can be generalized to the larger population. Examples: T-tests, ANOVA, regression analysis, and chi-squared tests are commonly used inferential models.

* Predictive Models: these are designed to make predictions about future or unseen data based on patterns and relationships learned from historical data. They are used in forecasting, classification, and regression tasks, and are valuable in applications such as weather forecasting, customer churn prediction, and stock price forecasting.
Examples: Linear regression, decision trees, random forests, neural networks, and support vector machines are popular predictive modeling techniques.

In summary, descriptive models help summarize and visualize data, inferential models draw conclusions about populations, and predictive models make future predictions based on historical data. The choice of model depends on your specific goals and the type of analysis you need to perform, whether it's for understanding data, drawing statistical inferences, or making predictions for decision-making.

```{r}

```


```{r}

```


```{r}

```





















